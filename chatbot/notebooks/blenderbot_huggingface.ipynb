{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Source:\n",
    "\n",
    " - Blenderbot Model Doc https://huggingface.co/docs/transformers/model_doc/blenderbot\n",
    "\n",
    " - Blenderbot model implementation https://github.com/huggingface/transformers/blob/master/src/transformers/models/blenderbot/modeling_blenderbot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt2 to instantiate a model of type blenderbot. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000002?line=8'>9</a>\u001b[0m \u001b[39m# mname = \"facebook/blenderbot-400M-distill\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000002?line=9'>10</a>\u001b[0m mname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmicrosoft/DialoGPT-small\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000002?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m BlenderbotForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(mname)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000002?line=11'>12</a>\u001b[0m \u001b[39m# model = BlenderbotModel.from_pretrained(mname)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000002?line=12'>13</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BlenderbotTokenizer\u001b[39m.\u001b[39mfrom_pretrained(mname)\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:1227\u001b[0m, in \u001b[0;36mBlenderbotForConditionalGeneration.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1220'>1221</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1221'>1222</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe checkpoint `facebook/blenderbot-90M` is deprecated. In the future, please use the identical checkpoint `facebook/small_blenderbot-90M` with `BlenderbotSmallForConditionalGeneration.from_pretrained(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfacebook/small_blenderbot-90M\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)` instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1222'>1223</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1223'>1224</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1224'>1225</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m BlenderbotSmallForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1226'>1227</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(BlenderbotForConditionalGeneration, \u001b[39mcls\u001b[39;49m)\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1227'>1228</a>\u001b[0m     pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1228'>1229</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/modeling_utils.py:1493\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1490'>1491</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1491'>1492</a>\u001b[0m     \u001b[39mwith\u001b[39;00m no_init_weights(_enable\u001b[39m=\u001b[39m_fast_init):\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1492'>1493</a>\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1494'>1495</a>\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1495'>1496</a>\u001b[0m     \u001b[39m# restore default dtype\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/modeling_utils.py?line=1496'>1497</a>\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:1211\u001b[0m, in \u001b[0;36mBlenderbotForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1208'>1209</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: BlenderbotConfig):\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1209'>1210</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1210'>1211</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m BlenderbotModel(config)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1211'>1212</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_buffer(\u001b[39m\"\u001b[39m\u001b[39mfinal_logits_bias\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mshared\u001b[39m.\u001b[39mnum_embeddings)))\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1212'>1213</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mshared\u001b[39m.\u001b[39mnum_embeddings, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:1072\u001b[0m, in \u001b[0;36mBlenderbotModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1068'>1069</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(vocab_size, config\u001b[39m.\u001b[39md_model, padding_idx)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1070'>1071</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BlenderbotEncoder(config, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared)\n\u001b[0;32m-> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1071'>1072</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m BlenderbotDecoder(config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshared)\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1073'>1074</a>\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=1074'>1075</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:811\u001b[0m, in \u001b[0;36mBlenderbotDecoder.__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=804'>805</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=806'>807</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_positions \u001b[39m=\u001b[39m BlenderbotLearnedPositionalEmbedding(\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=807'>808</a>\u001b[0m     config\u001b[39m.\u001b[39mmax_position_embeddings,\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=808'>809</a>\u001b[0m     config\u001b[39m.\u001b[39md_model,\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=809'>810</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=810'>811</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BlenderbotDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mdecoder_layers)])\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=811'>812</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39md_model)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=813'>814</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:811\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=804'>805</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39md_model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=806'>807</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_positions \u001b[39m=\u001b[39m BlenderbotLearnedPositionalEmbedding(\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=807'>808</a>\u001b[0m     config\u001b[39m.\u001b[39mmax_position_embeddings,\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=808'>809</a>\u001b[0m     config\u001b[39m.\u001b[39md_model,\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=809'>810</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=810'>811</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BlenderbotDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mdecoder_layers)])\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=811'>812</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(config\u001b[39m.\u001b[39md_model)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=813'>814</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:367\u001b[0m, in \u001b[0;36mBlenderbotDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=364'>365</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_attn_layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=365'>366</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, config\u001b[39m.\u001b[39mdecoder_ffn_dim)\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=366'>367</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mdecoder_ffn_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim)\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/transformers/models/blenderbot/modeling_blenderbot.py?line=367'>368</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py:90\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=87'>88</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=88'>89</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=92'>93</a>\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=93'>94</a>\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=94'>95</a>\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m---> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=95'>96</a>\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=96'>97</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/modules/linear.py?line=97'>98</a>\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/init.py:410\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/init.py?line=407'>408</a>\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/init.py?line=408'>409</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///~/miniforge3/envs/chatbot/lib/python3.9/site-packages/torch/nn/init.py?line=409'>410</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BlenderbotTokenizer\n",
    "\n",
    "# Blenderbot without language model head\n",
    "from transformers import BlenderbotModel\n",
    "\n",
    "# Blenderbot with a language model head\n",
    "from transformers import BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "# model = BlenderbotModel.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000004?line=3'>4</a>\u001b[0m UTTERANCE \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHello. How are you?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39m# UTTERANCE = \"My friends are cool but they eat too many carbs.\" * 2\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000004?line=6'>7</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer([UTTERANCE], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/frank/projects/dsr/projects/chatbot/chatbot/notebooks/blenderbot_huggingface.ipynb#ch0000004?line=7'>8</a>\u001b[0m inputs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "# UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "\n",
    "UTTERANCE = \"Hello. How are you?\"\n",
    "# UTTERANCE = \"My friends are cool but they eat too many carbs.\" * 2\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_ids = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:   I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\n"
     ]
    }
   ],
   "source": [
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "# mname = \"microsoft/DialoGPT-small\"\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "from transformers import BlenderbotTokenizer\n",
    "\n",
    "# Blenderbot without language model head\n",
    "from transformers import BlenderbotModel\n",
    "\n",
    "# Blenderbot with a language model head\n",
    "from transformers import BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "# model = BlenderbotModel.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[6950,   21,  855,  366,  304,   38,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "Bot:   I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<s> I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?</s>\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTTERANCE = \"Hello. How are you?\"\n",
    "# UTTERANCE = \"My friends are cool but they eat too many carbs.\" * 2\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "tokenizer.batch_decode(reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    13,  1374,   389,   345,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "Bot:  Hello. How are you? Friend zone and how are you friends?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello. How are you? Friend zone and how are you friends?<|endoftext|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTTERANCE = \"Hello. How are you?\"\n",
    "# UTTERANCE = \"My friends are cool but they eat too many carbs.\" * 2\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "tokenizer.batch_decode(reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6950,   21,  855,  366,  304,   38,    2,   72,   51,   72,   34,   74,\n",
       "           50,   82,   51,   75,   36,   64,   51,   75]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[6950,   21,  855,  366,  304,   38,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?</s>\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(reply_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual chat with blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:   Hello! What is your favorite dog breed?</s>\n",
      "tensor([[   1,  281,  446,  342,  360,  265, 2297, 4445,   19,  373,  281,  913,\n",
      "          430, 3914,   21,  228,  714,  458,  304,   38,    2]])\n",
      "Bot:  <s> I don't have a favorite breed, but I love all dogs.  What about you?</s>\n",
      "Input:   My favorite dog is a poodle</s>\n",
      "tensor([[   1,  281,  913,  286,  495,  917,  618,    8,  281,  360,  884,  306,\n",
      "          494,   21,  689,  366,  394, 2941,  298,  294,  347,  308,   21,    2]])\n",
      "Bot:  <s> I love poodles too! I have two of them. They are so smart and gentle.</s>\n"
     ]
    }
   ],
   "source": [
    "# chat_history = []\n",
    "\n",
    "input_text = \"Hello! What is your favorite dog breed?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(\"Input: \", tokenizer.decode(input_ids[0]))\n",
    "\n",
    "chat_history = torch.cat([input_ids], dim=-1)\n",
    "\n",
    "reply_ids = model.generate(chat_history, max_length=1250,)\n",
    "print(reply_ids)\n",
    "print(\"Bot: \", tokenizer.decode(reply_ids[0]))\n",
    "\n",
    "chat_history = torch.cat([chat_history, reply_ids], dim=-1)\n",
    "\n",
    "input_text = \"My favorite dog is a poodle\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(\"Input: \", tokenizer.decode(input_ids[0]))\n",
    "\n",
    "chat_history = torch.cat([chat_history, input_ids], dim=-1)\n",
    "\n",
    "reply_ids = model.generate(chat_history, max_length=1250,)\n",
    "print(reply_ids)\n",
    "print(\"Bot: \", tokenizer.decode(reply_ids[0]))\n",
    "\n",
    "\n",
    "# tokenizer.decode(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlenderbotTokenizer\n",
    "from transformers import BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n",
    "\n",
    "def add_user_input_to_chat_history(text_input, chat_history_ids, tokenizer):\n",
    "    \"\"\" Add user input to chat history \"\"\"    \n",
    "    # Encode new input\n",
    "    text_input_ids = tokenizer.encode(text_input, return_tensors='pt')\n",
    "    # Add to chat history\n",
    "    chat_history_ids = add_to_chat_history(text_input_ids, chat_history_ids)\n",
    "    return chat_history_ids\n",
    "\n",
    "\n",
    "def add_response_to_chat_history(chat_history_ids, model):\n",
    "    \"\"\" Generate a response to the chat history input\"\"\"\n",
    "    reply_ids = model.generate(chat_history_ids, max_length=1250,)\n",
    "    chat_history_ids = add_to_chat_history(reply_ids, chat_history_ids)\n",
    "    return chat_history_ids\n",
    "\n",
    "\n",
    "def add_to_chat_history(text_ids, chat_history_ids=None):\n",
    "    \"\"\" Add some text ids to chat history \"\"\"\n",
    "    # If chat history is empty, return empty\n",
    "    if chat_history_ids is None:\n",
    "        return text_ids\n",
    "    chat_history = torch.cat([chat_history_ids, text_ids], dim=-1)\n",
    "    return chat_history\n",
    "\n",
    "def print_chat_history(chat_history_ids):\n",
    "    chat_history = tokenizer.decode(chat_history_ids[0])\n",
    "    print(chat_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi! How are you Blendy? How is the weather?</s><s> Hi! I am doing well. The weather is great here. How about where you are?</s> I think I'll go dancing. I love R&B music.</s><s> I'm doing great! I love dancing too! What kind of dancing do you like to do?</s> I love dogs. What is your favorite dog breed?</s><s> I love all kinds of dogs, but my favorite is probably a Labrador Retriever. What about you?</s>\n"
     ]
    }
   ],
   "source": [
    "text_input = \"Hi! How are you Blendy? How is the weather?\"\n",
    "chat_history_ids = add_user_input_to_chat_history(text_input, None, tokenizer)\n",
    "chat_history_ids = add_response_to_chat_history(chat_history_ids, model)\n",
    "\n",
    "text_input = \"I think I'll go dancing. I love R&B music.\"\n",
    "chat_history_ids = add_user_input_to_chat_history(text_input, chat_history_ids, tokenizer)\n",
    "chat_history_ids = add_response_to_chat_history(chat_history_ids, model)\n",
    "\n",
    "text_input = \"I love dogs. What is your favorite dog breed?\"\n",
    "chat_history_ids = add_user_input_to_chat_history(text_input, chat_history_ids, tokenizer)\n",
    "chat_history_ids = add_response_to_chat_history(chat_history_ids, model)\n",
    "\n",
    "\n",
    "print_chat_history(chat_history_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello! What is your favorite dog breed?</s><s> I don't have a favorite breed, but I love all dogs.  What about you?</s> My favorite dog is a poodle</s>\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def generate_response(tokenizer, model, chat_round, chat_history_ids):\n",
    "    \"\"\" Generate a response to user input\n",
    "    \"\"\"\n",
    "    # Get user input and EOS token\n",
    "    # new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\"), return_tensors='pt')\n",
    "\n",
    "    # Append to chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\\\n",
    "        if chat_round>0 else new_user_input_ids\n",
    "    \n",
    "    # Generate response given maximum chat length history of 1250 tokens(?)\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=1250, # Total chat history\n",
    "        # pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Pretty print out tokens from the bot\n",
    "    print(\"Blenderbot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "    return chat_history_ids\n",
    "\n",
    "def chat_for_n_rounds(n=5):\n",
    "    \"\"\" Chat with the chatbot for n rounds\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    # tokenizer, model = load_tokenizer_and_model\n",
    "      # Initialize history variable\n",
    "    chat_history_ids = None\n",
    "    \n",
    "    # Chat for n rounds\n",
    "    for chat_round in range(n):\n",
    "        chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc8015e61a03201b035b34b1113fcdaafe270f77fdb5ef38e87d79416051dc2f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('chatbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
