{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Source:\n",
    "\n",
    " - Blenderbot Model Doc https://huggingface.co/docs/transformers/model_doc/blenderbot\n",
    "\n",
    " - Blenderbot model implementation https://github.com/huggingface/transformers/blob/master/src/transformers/models/blenderbot/modeling_blenderbot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n",
    "# mname = \"microsoft/DialoGPT-small\"\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "from transformers import BlenderbotTokenizer\n",
    "\n",
    "# Blenderbot without language model head\n",
    "from transformers import BlenderbotModel\n",
    "\n",
    "# Blenderbot with a language model head\n",
    "from transformers import BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "# model = BlenderbotModel.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[6950,   21,  855,  366,  304,   38,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "Bot:   I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<s> I'm doing well, thank you. How about yourself? Do you have any plans for the weekend?</s>\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTTERANCE = \"Hello. How are you?\"\n",
    "# UTTERANCE = \"My friends are cool but they eat too many carbs.\" * 2\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "tokenizer.batch_decode(reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    13,  1374,   389,   345,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "Bot:  Hello. How are you? Friend zone and how are you friends?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello. How are you? Friend zone and how are you friends?<|endoftext|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTTERANCE = \"Hello. How are you?\"\n",
    "# UTTERANCE = \"My friends are cool but they eat too many carbs.\" * 2\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "tokenizer.batch_decode(reply_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual chat with blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  </s>\n",
      "tensor([[2]])\n",
      "tensor([[   1,    1,  946,  304,  360,  463,  286, 1272,   38,  281,  360,  265,\n",
      "         1784,  298,  338,  341,  395,  899,  903,   21,    2]])\n",
      "Bot:  <s><s> Do you have any pets? I have a dog and he's my best friend.</s>\n",
      "Input:   My favorite dog is a poodle</s>\n",
      "tensor([[   1,  281,  913,  286,  495,  917,    8,  228,  281,  360,  884,  306,\n",
      "          494,   21,  228,  714,  906,  306, 1784,  361,  304,  360,   38,    2]])\n",
      "Bot:  <s> I love poodles!  I have two of them.  What kind of dog do you have?</s>\n"
     ]
    }
   ],
   "source": [
    "# chat_history = []\n",
    "\n",
    "input_text = \"Hello! What is your favorite dog breed?\"\n",
    "# input_text = \"\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(\"Input: \", tokenizer.decode(input_ids[0]))\n",
    "print(input_ids)\n",
    "\n",
    "chat_history = torch.cat([input_ids], dim=-1)\n",
    "\n",
    "reply_ids = model.generate(chat_history, max_length=1250,)\n",
    "print(reply_ids)\n",
    "print(\"Bot: \", tokenizer.decode(reply_ids[0]))\n",
    "\n",
    "chat_history = torch.cat([chat_history, reply_ids], dim=-1)\n",
    "\n",
    "input_text = \"My favorite dog is a poodle\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(\"Input: \", tokenizer.decode(input_ids[0]))\n",
    "\n",
    "chat_history = torch.cat([chat_history, input_ids], dim=-1)\n",
    "\n",
    "reply_ids = model.generate(chat_history, max_length=1250,)\n",
    "print(reply_ids)\n",
    "print(\"Bot: \", tokenizer.decode(reply_ids[0]))\n",
    "\n",
    "\n",
    "# tokenizer.decode(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I love poodles too! I have two of them. They are so smart and gentle.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(reply_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlenderbotTokenizer\n",
    "from transformers import BlenderbotForConditionalGeneration\n",
    "\n",
    "mname = \"facebook/blenderbot-400M-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n",
    "\n",
    "def add_user_input_to_chat_history(text_input, chat_history_ids, tokenizer):\n",
    "    \"\"\" Add user input to chat history \"\"\"    \n",
    "    # Encode new input\n",
    "    text_input_ids = tokenizer.encode(text_input, return_tensors='pt')\n",
    "    # Add to chat history\n",
    "    chat_history_ids = add_to_chat_history(text_input_ids, chat_history_ids)\n",
    "    return chat_history_ids\n",
    "\n",
    "\n",
    "def add_response_to_chat_history(chat_history_ids, model):\n",
    "    \"\"\" Generate a response to the chat history input\"\"\"\n",
    "    reply_ids = model.generate(chat_history_ids, max_length=1250,)\n",
    "    chat_history_ids = add_to_chat_history(reply_ids, chat_history_ids)\n",
    "    return chat_history_ids\n",
    "\n",
    "\n",
    "def add_to_chat_history(text_ids, chat_history_ids=None):\n",
    "    \"\"\" Add some text ids to chat history \"\"\"\n",
    "    # If chat history is empty, return empty\n",
    "    if chat_history_ids is None:\n",
    "        return text_ids\n",
    "    chat_history = torch.cat([chat_history_ids, text_ids], dim=-1)\n",
    "    return chat_history\n",
    "\n",
    "def print_chat_history(chat_history_ids):\n",
    "    chat_history = tokenizer.decode(chat_history_ids[0])\n",
    "    print(chat_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi! How are you Blendy? How is the weather?</s><s> Hi! I am doing well. The weather is great here. How about where you are?</s> I think I'll go dancing. I love R&B music.</s><s> I'm doing great! I love dancing too! What kind of dancing do you like to do?</s> I love dogs. What is your favorite dog breed?</s><s> I love all kinds of dogs, but my favorite is probably a Labrador Retriever. What about you?</s>\n"
     ]
    }
   ],
   "source": [
    "text_input = \"Hi! How are you Blendy? How is the weather?\"\n",
    "chat_history_ids = add_user_input_to_chat_history(text_input, None, tokenizer)\n",
    "chat_history_ids = add_response_to_chat_history(chat_history_ids, model)\n",
    "\n",
    "text_input = \"I think I'll go dancing. I love R&B music.\"\n",
    "chat_history_ids = add_user_input_to_chat_history(text_input, chat_history_ids, tokenizer)\n",
    "chat_history_ids = add_response_to_chat_history(chat_history_ids, model)\n",
    "\n",
    "text_input = \"I love dogs. What is your favorite dog breed?\"\n",
    "chat_history_ids = add_user_input_to_chat_history(text_input, chat_history_ids, tokenizer)\n",
    "chat_history_ids = add_response_to_chat_history(chat_history_ids, model)\n",
    "\n",
    "\n",
    "print_chat_history(chat_history_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello! What is your favorite dog breed?</s><s> I don't have a favorite breed, but I love all dogs.  What about you?</s> My favorite dog is a poodle</s>\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def generate_response(tokenizer, model, chat_round, chat_history_ids):\n",
    "    \"\"\" Generate a response to user input\n",
    "    \"\"\"\n",
    "    # Get user input and EOS token\n",
    "    # new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\"), return_tensors='pt')\n",
    "\n",
    "    # Append to chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\\\n",
    "        if chat_round>0 else new_user_input_ids\n",
    "    \n",
    "    # Generate response given maximum chat length history of 1250 tokens(?)\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=1250, # Total chat history\n",
    "        # pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Pretty print out tokens from the bot\n",
    "    print(\"Blenderbot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "    return chat_history_ids\n",
    "\n",
    "def chat_for_n_rounds(n=5):\n",
    "    \"\"\" Chat with the chatbot for n rounds\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    # tokenizer, model = load_tokenizer_and_model\n",
    "      # Initialize history variable\n",
    "    chat_history_ids = None\n",
    "    \n",
    "    # Chat for n rounds\n",
    "    for chat_round in range(n):\n",
    "        chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc8015e61a03201b035b34b1113fcdaafe270f77fdb5ef38e87d79416051dc2f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('chatbot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
