{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with DialoGPT Experiments\n",
    "\n",
    "\n",
    "Resouces \n",
    " - https://towardsdatascience.com/make-your-own-rick-sanchez-bot-with-transformers-and-dialogpt-fine-tuning-f85e6d1f4e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 11.6kB/s]\n",
      "Downloading: 100%|██████████| 641/641 [00:00<00:00, 473kB/s]\n",
      "Downloading: 100%|██████████| 0.99M/0.99M [00:00<00:00, 1.47MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 830kB/s] \n",
      "/Users/frank/miniforge3/envs/ml-base/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:742: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Downloading: 100%|██████████| 335M/335M [00:28<00:00, 12.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_tokenizer_and_model(model=\"microsoft/DialoGPT-large\"):\n",
    "#   \"\"\"\n",
    "#     Load tokenizer and model instance for some specific DialoGPT model.\n",
    "#   \"\"\"\n",
    "#   # Initialize tokenizer and model\n",
    "#   print(\"Loading model...\")\n",
    "#   tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "#   model = AutoModelForCausalLM.from_pretrained(model)\n",
    "  \n",
    "  # Return tokenizer and model\n",
    "#   return tokenizer, model\n",
    "# Make chat with user input\n",
    "def generate_response(tokenizer, model, chat_round, chat_history_ids):\n",
    "    \"\"\" Generate a response to user input\n",
    "    \"\"\"\n",
    "    # Get user input and EOS token\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # Append to chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\\\n",
    "        if chat_round>0 else new_user_input_ids\n",
    "    \n",
    "    # Generate response given maximum chat length history of 1250 tokens(?)\n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Pretty print out tokens from the bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "    return chat_history_ids\n",
    "\n",
    "def chat_for_n_rounds(n=5):\n",
    "    \"\"\" Chat with the chatbot for n rounds\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    # tokenizer, model = load_tokenizer_and_model\n",
    "      # Initialize history variable\n",
    "    chat_history_ids = None\n",
    "    \n",
    "    # Chat for n rounds\n",
    "    for chat_round in range(n):\n",
    "        chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: Hi, I'm a guy.\n",
      "DialoGPT: I'm a guy.\n",
      "DialoGPT: I'm a girl.\n",
      "DialoGPT: I'm a girl.\n",
      "DialoGPT: I'm a girl.\n"
     ]
    }
   ],
   "source": [
    "chat_for_n_rounds(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 50256]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([new_user_input_ids, new_user_input_ids], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81e4e442e8f1a6312a7121210649192385821529460aee73e726d2e04cd8ba6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml-base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
